{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#46BAED;overflow:hidden;font-weight:bold\">TPS April 2022</div>\n\n# <span style=\"color:#46BAED;margin:0;text-align:left;font-weight:bold\">Competition Objective</span>\nThis month's [TPS Competition](https://www.kaggle.com/competitions/tabular-playground-series-apr-2022) is a time series classification problem. The goal of this competition is to identify whether a subject is in Activity State 0 or 1 based on 60-second sequences of biological sensor data recorded from several hundred participants.\n\n# <span style=\"color:#46BAED;margin:0;text-align:left;font-weight:bold\">Data Overview</span>\nThe training set comprises ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants. The variables in the dataset include:\n- `sequence`: a unique ID for each sequence.\n- `subject`: a unique ID for each subject in the experiment.\n- `step`: the time step of the recording, in one second intervals.\n- `sensor_00` - `sensor_12`: the value for each of the thirteen sensors at that time step.\n\nThe train_labels.csv contains:\n- `sequence`: the unique ID for each sequence.\n- `state`: the state associated to each sequence, our target variable.\n\nThe final predictions for each participant's activity state will be made based on the sensor information for ~12,000 sequences in the test set.","metadata":{}},{"cell_type":"code","source":"import warnings, gc\nimport numpy as np \nimport pandas as pd\nimport matplotlib as mpl\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\nfrom scipy.stats import gaussian_kde\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import KFold, GroupKFold \nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom lightgbm import LGBMClassifier\n\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\ncolor=px.colors.qualitative.Plotly\ntemp=dict(layout=go.Layout(font=dict(family=\"Franklin Gothic\", size=12), \n                           height=500, width=700))\n\ntrain=pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\nsub=pd.read_csv('../input/tabular-playground-series-apr-2022/sample_submission.csv')\nlbl=pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\n\nprint(\"Train Shape: There are {:,.0f} rows and {:,.0f} columns.\\nMissing values = {}, Duplicates = {}.\\n\".\n      format(train.shape[0], train.shape[1],train.isna().sum().sum(), train.duplicated().sum()))\nprint(\"There are {:,.0f} training labels.\\n\".format(lbl.shape[0]))\nprint(\"Test Shape: There are {:,.0f} rows and {:,.0f} columns.\\nMissing values = {}, Duplicates = {}.\\n\".\n      format(test.shape[0], test.shape[1], test.isna().sum().sum(), test.duplicated().sum()))\n\ntrain=train.merge(lbl, on='sequence')\ndf0=train[train.state==0].describe().reset_index()\ndf1=train[train.state==1].describe().reset_index()\ndf0['state']=0\ndf1['state']=1\ndf=pd.concat([df0,df1], axis=0).set_index(['index','state'])\ndf.index = df.index.set_names(['','State'])\ndf=df.reindex([('count', 0),('count', 1),( 'mean', 0),( 'mean', 1),\n               (  'std', 0), (  'std', 1),(  'min', 0),(  'min', 1),\n               (  '25%', 0), (  '25%', 1),(  '50%', 0),(  '50%', 1),\n               (  '75%', 0),(  '75%', 1),(  'max', 0),(  'max', 1)])\ndisplay(df.style.format('{:,.2f}')\n        .background_gradient(subset=(df.index[2:],df.columns[:]), cmap='GnBu', axis=0))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T04:44:33.360169Z","iopub.execute_input":"2022-04-18T04:44:33.360552Z","iopub.status.idle":"2022-04-18T04:44:51.602096Z","shell.execute_reply.started":"2022-04-18T04:44:33.360423Z","shell.execute_reply":"2022-04-18T04:44:51.601296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style=\"color:#46BAED;margin:0;text-align:left\">Exploratory Data Analysis</span></b>","metadata":{}},{"cell_type":"code","source":"p1=pd.DataFrame.from_dict({'Train': train.subject.nunique(), 'Test': test.subject.nunique()}, \n                          orient='index', columns=['count'])\np2=pd.DataFrame.from_dict({'Train': train.sequence.nunique(), 'Test': test.sequence.nunique()}, \n                          orient='index', columns=['count'])\np3=pd.DataFrame.from_dict({'Train':(train.groupby('subject')['step'].count().sum()/train.subject.nunique())/60, \n                           'Test':(test.groupby('subject')['step'].count().sum()/test.subject.nunique())/60}, \n                          orient='index', columns=['count'])\ncol_list=['#0969AD','#F2BB08']\nrgb=['rgba'+str(mpl.colors.to_rgba(i, 0.55)) for i in col_list]\ntitles=['Total Participants', '60-Second Sequences',\n        'Average Time per Participant']\nfig = make_subplots(rows=1, cols=3, \n                    subplot_titles=titles)\ntext=['','',' min']\nlegend=True\nfor i, df in enumerate([p1,p2,p3]):\n    if i!=0: legend=False\n    for j in range(0,2):\n        fig.add_trace(go.Bar(x=[df.index[j]], y=df.iloc[j,:], \n                             text=df.iloc[j,:], name=df.index[j], width=0.8,\n                             texttemplate='%{text:,.0f}'+str(text[i]), textposition='outside',\n                             marker=dict(color=rgb[j], line_color=col_list[j], line_width=2),\n                             hovertemplate='%{x} '+str(titles[i])+' = %{y:,.0f}'+str(text[i]),\n                             showlegend=legend),row=1,col=i+1)\nfig.update_layout(template=temp, title='Train and Test Set Overview', barmode='group',\n                  yaxis1_range=(0,730),yaxis2_range=(0,28000),yaxis3_range=(0,42),\n                  legend=dict(orientation=\"h\",yanchor=\"bottom\",xanchor=\"right\",\n                              y=1.14,x=.96,traceorder='normal'))\nfig.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T04:48:22.186727Z","iopub.execute_input":"2022-04-18T04:48:22.18705Z","iopub.status.idle":"2022-04-18T04:48:22.554057Z","shell.execute_reply.started":"2022-04-18T04:48:22.187017Z","shell.execute_reply":"2022-04-18T04:48:22.553515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 672 participants in the training set with just under 26,000 unique sequences. In the test set, we have 319 participants with about 12,000 sequences. The average length of time for each participantâ€™s sensor data is around 39 minutes in both sets.","metadata":{}},{"cell_type":"code","source":"rgb=['rgba'+str(mpl.colors.to_rgba(i, 0.5)) for i in color[:2]]\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{},{}],\n                           [{\"colspan\":2}, None]],\n                    vertical_spacing=0.18,\n                    column_widths=(.25,.75),\n                    row_heights=(.55,.45),\n                    subplot_titles=('Target Distribution', \n                                    'Unique Sequences per Participant', \n                                    'Proportion of time spent in each state'))\n\nplot1=train.state.value_counts(normalize=True).mul(100).reset_index()\nplot1['index']=plot1['index'].apply(lambda x: 'State 0' if x==0 else 'State 1')\nfig.add_trace(go.Bar(x=plot1['index'], y=plot1['state'], text=plot1['state'],\n                     texttemplate='%{text:.1f}%', textposition='outside',\n                     marker=dict(color=rgb[::-1], line_color=color[:2][::-1], \n                                 line_width=2),\n                     hovertemplate='%{x} = %{y:.1f}%<extra></extra>',\n                     showlegend=False),row=1,col=1)\n\nplot2=train.groupby(['subject','state'])['sequence'].nunique().reset_index()\nfor i in reversed(range(0,2)): \n    x=plot2[plot2.state==i]['subject']\n    y=plot2[plot2.state==i]['sequence']\n    fig.add_trace(go.Scatter(x=x, y=y, name='State {}'.format(i), mode='lines', \n                             marker_color=color[i],showlegend=False), \n                  row=1,col=2)\n\nplot3=train.groupby(['subject','state'])['sequence'].nunique()\nplot3=plot3.div(plot3.sum(level=0), level=0).mul(100).reset_index()\np0=plot3[plot3.state==0].sort_values(by='sequence').reset_index(drop=True)\np1=plot3[plot3.state==1].sort_values(by='sequence',ascending=False).reset_index(drop=True)\nfill=['tozeroy','tonexty']\nfor i, df in enumerate([p1,p0]):\n    fig.add_trace(go.Scatter(x=df.index, y=df.sequence, fill=fill[i], \n                             stackgroup='one',name='State {}'.format(df.state.max()),\n                             marker_color=color[:2][::-1][i]),row=2,col=1)\nfig.update_xaxes(showline=False, zeroline=False)\nfig.update_layout(template=temp, height=800, hovermode='x unified',\n                  yaxis1=dict(range=(0,58),ticksuffix='%'),\n                  xaxis2_title='Participant',\n                  yaxis3_ticksuffix='%', xaxis3_title=\"Participant\",\n                  legend=dict(orientation=\"v\",yanchor=\"bottom\",xanchor=\"right\",\n                              y=.36,x=1,traceorder='normal'))\nfig.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T04:48:28.808994Z","iopub.execute_input":"2022-04-18T04:48:28.809414Z","iopub.status.idle":"2022-04-18T04:48:29.318134Z","shell.execute_reply.started":"2022-04-18T04:48:28.809385Z","shell.execute_reply":"2022-04-18T04:48:29.317273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our target variable is evenly balanced with about 50% of sequences in each state. When we look at the number of sequences for each participant, the total amount in State 0 tends to stay under 50, while some participants are in State 1 for more than 100 sequences. Interestingly, we also have about 60 participants who never go into State 1. ","metadata":{}},{"cell_type":"code","source":"sensors=[col for col in train if col.startswith('sensor')]\nrowtitles=['Sensor {}'.format(sensors[i][-2:]) for i in range(len(sensors))]\nt1=['Distribution of Sensor {}'.format(sensors[i][-2:]) for i in range(len(sensors))]\nt2=['Median Signals by Second' for i in range(len(sensors))]\ntitles=[x for y in zip(t1, t2) for x in y]\n\nfig = make_subplots(rows=13, cols=2, \n                    horizontal_spacing=0.1, \n                    subplot_titles=titles, \n                    row_titles=rowtitles)\n\nlegend=True\nfor i, sensor in enumerate(sensors):\n    if i != 0:  \n        legend=False\n    for j in range(0,2):\n        hist_data=train[train.state==j][sensor]\n        density=gaussian_kde(dataset=hist_data)\n        x=np.arange(hist_data.min(), hist_data.max()) \n        density.covariance_factor = lambda: 4  \n        density._compute_covariance()\n        kde_curve=density(x)\n        fig.append_trace(go.Scatter(x=x, y=kde_curve, marker_color=color[j], \n                                    fill='tozeroy', name='State {}'.format(j), \n                                    hovertemplate='Density = %{y:.5f} at %{x}',\n                                    showlegend=legend), \n                         row=i+1, col=1)\n        fig.update_yaxes(title=\"Density\",row=i+1, col=1)\n        \n    plot_df=train.groupby(['step','state'])[sensor].median().reset_index()\n    for j in reversed(range(0,2)): \n        y=plot_df[plot_df.state==j][sensor]\n        x=plot_df.step.unique()\n        if y.sum() != 0:\n            fig.append_trace(go.Bar(x=x, y=y, name='State {}'.format(j),\n                                    marker_color=color[j], \n                                    hovertemplate='Median Signal = %{y:.5f} at second %{x}',\n                                    opacity=0.9, showlegend=False),\n                             row=i+1, col=2)\n            fig.update_xaxes(title='Time, in seconds',row=i+1, col=2)\n            fig.update_yaxes(title=\"Signal\",row=i+1, col=2)\n        else:\n            fig.append_trace(go.Scatter(x=x, y=y, name='State {}'.format(j),\n                                        marker_color=color[j], \n                                        showlegend=False),\n                             row=i+1, col=2) \n            fig.update_xaxes(title='Time, in seconds',row=i+1, col=2)\n            fig.update_yaxes(title=\"Signal\",row=i+1, col=2)\nfig.update_layout(template=temp, title='Sensor Signal Distributions', \n                  barmode='relative', height=4000, \n                  legend=dict(orientation=\"v\",yanchor=\"bottom\",\n                              xanchor=\"right\",y=1.01,x=.99,\n                              traceorder='reversed'))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:48:36.87572Z","iopub.execute_input":"2022-04-18T04:48:36.876366Z","iopub.status.idle":"2022-04-18T04:52:57.087693Z","shell.execute_reply.started":"2022-04-18T04:48:36.876331Z","shell.execute_reply":"2022-04-18T04:52:57.086815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both activity states, the majority of the sensors have long, heavy-tailed distributions with a peak centered at 0, except Sensors 4 and 12, which more closely resemble a Gaussian distribution, and Sensor 2, which has a left-skewed distribution. In the graphs of the median values at each second, Sensor 2's median signal is 0 across all 60 time steps for both states. This also occurs in Sensor 8, while the rest of the sensors show State 1 tends to have greater median values than State 0 in both positive and negative directions at each time step. ","metadata":{}},{"cell_type":"code","source":"sensors=[col for col in train if col.startswith('sensor')]\ncorr=train.iloc[:,3:-1].corr(method='spearman')\nt=train[train.subject==1]\nx,y=[-3,0,0,3],[-1.5,0.25,0,1.5]\nxbins=dict(start=-3, end=3, size=0.2)\nybins=dict(start=-1.5, end=1.5, size=0.2)\nax=dict(zeroline=False, showgrid=False,\n        mirror=True,showline=True,linecolor='#F1F1F1')\ncmap=mpl.colors.LinearSegmentedColormap.from_list(\"\", \n                                                  [\"#2290AC\", '#B4DAEA', '#FFB3B3',\"#E92D22\"])\ncol_scale=[[0, 'white'], [0.25, 'rgb(10,136,186)'], \n           [0.5, 'rgb(242,211,56)'],[0.75, 'rgb(242,143,56)'],\n           [1, 'rgb(217,30,30)']]\n\nfig = make_subplots(rows=13, cols=13, \n                    shared_yaxes=True)\n# Density plots\nfor i, sensor in enumerate(sensors):\n    x_hist=t[sensor]\n    for j, sen in enumerate(sensors[i:]):\n        fig.add_trace(go.Histogram2d(x=x_hist, y=t[sen], histnorm='', zsmooth='fast',\n                                     xbins=xbins,ybins=ybins, name='', showscale=False,\n                                     colorscale=col_scale), row=i+j+1,col=i+1)\n        fig.update_yaxes(ax,row=i+j+1,col=i+1)\n        fig.update_xaxes(ax,row=i+j+1,col=i+1)\n\n# Correlations\nfor i, sen in enumerate(sensors):\n    c=corr[sen][i+1:]\n    if len(c) > 1:\n        norm=mpl.colors.TwoSlopeNorm(vmin=corr.min().min()+.1, vcenter=0, vmax=corr.max().max()-.3)\n    sensor_cmap=cmap(norm(c)).tolist()\n    m=[mpl.colors.rgb2hex(sensor_cmap[rgb]) for rgb in range(len(sensor_cmap))]\n    n=i+2\n    for j in np.arange(i+2,14):\n        fig.add_trace(go.Scatter(x=x,y=y,mode=\"text\",text=[\"\",\"Corr:\", \"<br>{:.3f}\".format(c[j-n]), \"\"],\n                                 hovertemplate='Sensor {} and {} Correlation = {:.4f}'.format(i,j-n+i+1,c[j-n]),\n                                 name='',textfont_color=[\"#ffffff\",\"#555555\",\"{}\".format(m[j-n])],\n                                 showlegend=False), \n                      row=i+1,col=j)\n        fig.update_yaxes(ax,showticklabels=False, row=i+1,col=j)\n        fig.update_xaxes(ax,showticklabels=False, row=i+1,col=j)\nfig.update_layout(template=temp, height=1950, width=1000,\n                  title='Sensor Correlations and Density Plots', \n                  yaxis1_title='Sensor 0', yaxis14_title='Sensor 1',\n                  yaxis27_title='Sensor 2', yaxis40_title='Sensor 3', \n                  yaxis53_title='Sensor 4', yaxis66_title='Sensor 5',\n                  yaxis79_title='Sensor 6', yaxis92_title='Sensor 7',\n                  yaxis105_title='Sensor 8', yaxis118_title='Sensor 9',\n                  yaxis131_title='Sensor 10', yaxis144_title='Sensor 11',\n                  yaxis157_title='Sensor 12', xaxis157_title='Sensor 0',\n                  xaxis158_title='Sensor 1', xaxis159_title='Sensor 2',\n                  xaxis160_title='Sensor 3', xaxis161_title='Sensor 4',\n                  xaxis162_title='Sensor 5', xaxis163_title='Sensor 6',\n                  xaxis164_title='Sensor 7', xaxis165_title='Sensor 8',\n                  xaxis166_title='Sensor 9', xaxis167_title='Sensor 10',\n                  xaxis168_title='Sensor 11', xaxis169_title='Sensor 12')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:53:10.334363Z","iopub.execute_input":"2022-04-18T04:53:10.334657Z","iopub.status.idle":"2022-04-18T04:53:37.796144Z","shell.execute_reply.started":"2022-04-18T04:53:10.334628Z","shell.execute_reply":"2022-04-18T04:53:37.794772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking a closer look at the relationships between the Sensors, the highest positive correlations exist between Sensors 3 and 7 and Sensors 0 and 6 of 0.759 and 0.752, respectively. The density plots also display a strong linear relationship between these sensors. Additionally, there is a moderately strong association between Sensor 0 and 9 of 0.612 and Sensor 3 and 11 of 0.57, while Sensors 2, 8, and 12 are the least correlated with the rest of the sensors.\n\n# <b><span style=\"color:#46BAED;margin:0;text-align:left\">Feature Engineering</span></b>\nTo add additional information to the models, I will include lagged variables and rolling averages for each sensor and take the first difference between each time step. I will also add aggregated sensor information for each sequence and subject, including variables for the mean, median, standard deviation, skewness, kurtosis, minimum, and maximum. These variables will be used in the Logistic Regression and Gradient Boosting models.\nThe graphs below show the average observed signals and the first differenced signals at each time step for Participant 1. ","metadata":{}},{"cell_type":"code","source":"def feat_eng(df):\n    \n    seq_df=pd.DataFrame()\n    sensors=[col for col in df if col.startswith('sensor')]\n    \n    for sensor in sensors:\n        df['{}_lag1'.format(sensor)] = df.groupby('sequence')[sensor].shift(1)\n        df['{}_lag1'.format(sensor)].fillna(df[sensor].median(), inplace=True)\n        df['{}_diff'.format(sensor)] = df[sensor] - df['{}_lag1'.format(sensor)] \n        df['{}_roll_mean3'.format(sensor)]=df['{}'.format(sensor)].rolling(window=3).mean()\n        df['{}_roll_mean6'.format(sensor)]=df['{}'.format(sensor)].rolling(window=6).mean()\n        df['{}_roll_mean9'.format(sensor)]=df['{}'.format(sensor)].rolling(window=9).mean()\n        df['{}_roll_mean3'.format(sensor)].fillna(df['{}_roll_mean3'.format(sensor)].median(), inplace=True)\n        df['{}_roll_mean6'.format(sensor)].fillna(df['{}_roll_mean6'.format(sensor)].median(), inplace=True)\n        df['{}_roll_mean9'.format(sensor)].fillna(df['{}_roll_mean9'.format(sensor)].median(), inplace=True)\n        s_diff='{}_diff'.format(sensor)\n        seq_df['{}_mean'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].mean()\n        seq_df['{}_diff_mean'.format(sensor)] = df.groupby(['sequence','subject'])[s_diff].mean()\n        seq_df['{}_med'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].median()\n        seq_df['{}_std'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].std()\n        seq_df['{}_skew'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].skew()\n        seq_df['{}_kurt'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].apply(pd.DataFrame.kurt)\n        seq_df['{}_min'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].min()\n        seq_df['{}_max'.format(sensor)] = df.groupby(['sequence','subject'])[sensor].max()\n    \n    return df, seq_df.reset_index()\n\ntrain, seq_df=feat_eng(df=train)\ntest, test_seq_df=feat_eng(df=test)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T04:54:30.846374Z","iopub.execute_input":"2022-04-18T04:54:30.847215Z","iopub.status.idle":"2022-04-18T04:57:00.472236Z","shell.execute_reply.started":"2022-04-18T04:54:30.847154Z","shell.execute_reply":"2022-04-18T04:57:00.471497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t1=['Sensor {}  Observed Signals'.format(sensors[i][-2:]) for i in range(len(sensors))]\nt2=['Differenced Signals' for i in range(len(sensors))]\ntitles=[x for y in zip(t1, t2) for x in y]\n\nfig = make_subplots(rows=13, cols=2, shared_yaxes=True,\n                    horizontal_spacing=0.05, vertical_spacing=0.04,\n                    subplot_titles=titles)\n\nt=train[train.subject==1]\nfor i,sensor in enumerate(sensors): \n    x=np.arange(0,t.sequence.nunique())\n    y=t.groupby('sequence')[sensor].mean()\n    fig.add_trace(go.Scatter(x=x, y=y, name='Average Signal', \n                             mode='lines+markers',\n                             hovertemplate='%{y:.3f} at %{x} minutes',\n                             marker=dict(color='#3698CC', size=4), \n                             opacity=0.9, showlegend=False),\n                  row=i+1, col=1)\n    fig.update_yaxes(title='Signal'.format(i), row=i+1, col=1)\n    \nsensor_diff=[col for col in t.columns if 'diff' in col]\nfor i,sensor in enumerate(sensor_diff):\n    x=np.arange(0,t.sequence.nunique())\n    y=t.groupby('sequence')[sensor].mean()\n    fig.add_trace(go.Scatter(x=x, y=y, name='Differenced Signal', \n                             mode='lines+markers', \n                             hovertemplate='%{y:.3f} at %{x} minutes',\n                             marker=dict(color='#EE938D', size=4), \n                             showlegend=False),\n                  row=i+1, col=2)\nfig.update_xaxes(title='Time, in minutes')\nfig.update_layout(template=temp, title='Original vs. Differenced Sensor Signals', \n                  hovermode=\"x unified\", height=3500, \n                  legend=dict(orientation=\"v\",yanchor=\"bottom\",xanchor=\"right\",y=1.01,x=.97))\nfig.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T04:57:07.677053Z","iopub.execute_input":"2022-04-18T04:57:07.677336Z","iopub.status.idle":"2022-04-18T04:57:09.096355Z","shell.execute_reply.started":"2022-04-18T04:57:07.677305Z","shell.execute_reply":"2022-04-18T04:57:09.095312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style=\"color:#46BAED;margin:0;text-align:left\">Logistic Regression</span></b>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Concatenate, GRU\nfrom tensorflow.keras.layers import Input, Dense, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, GlobalMaxPooling1D\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import metrics, regularizers\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-18T04:57:18.873631Z","iopub.execute_input":"2022-04-18T04:57:18.873956Z","iopub.status.idle":"2022-04-18T04:57:30.062015Z","shell.execute_reply.started":"2022-04-18T04:57:18.873925Z","shell.execute_reply":"2022-04-18T04:57:30.061313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors=px.colors.qualitative.Vivid\ndef plot_roc(y_val, y_prob):\n    fig=make_subplots(rows=1, cols=3)\n    for i in range(len(y_val)):\n        y=y_val[i]\n        prob=y_prob[i]\n        fpr, tpr, thresh = roc_curve(y, prob)\n        roc_auc = auc(fpr,tpr)\n        fig.append_trace(go.Scatter(x=fpr, y=tpr, line=dict(color=colors[i+1], width=3), \n                                    hovertemplate = 'True positive rate = %{y:.3f}, False positive rate = %{x:.3f}',\n                                    name='Fold {} AUC = {:.3f}  '.format(i+1,roc_auc)), \n                         row=1,col=i+1)\n        fig.add_shape(type=\"line\", xref=\"x\", yref=\"y\", x0=0, y0=0, x1=1, y1=1, \n                      line=dict(color=\"Black\", width=1, dash=\"dot\"), row=1, col=i+1)\n        fig.update_xaxes(title='False Positive Rate')\n    fig.update_layout(template=temp, title=\"Cross-Validation ROC Curves\", \n                      hovermode=\"x unified\", height=400,\n                      yaxis_title='True Positive Rate (Sensitivity)', \n                      legend=dict(orientation='h',y=1.175, x=.5, xanchor=\"center\",\n                                  bordercolor=\"black\", borderwidth=.5, font=dict(size=12)))\n    fig.show()\n    \ndef plot_predictions(df, title):\n    plot_df=pd.DataFrame.from_dict({'State 1':(len(df[df.state>0.5])/len(df.state))*100, \n         'State 0':(len(df[df.state<=0.5])/len(df.state))*100}, orient='index', columns=['pct'])\n    fig=go.Figure()\n    col=['#F7AA9D','#B1B6FC']\n    text=['State 1', 'State 0']\n    fig.add_trace(go.Pie(labels=plot_df.index, values=plot_df['pct'], hole=.38, \n                         text=text, showlegend=False,\n                         marker=dict(colors=col,line=dict(color=color[:2][::-1],width=2)),\n                         hovertemplate = \"%{label}: %{value:.2f}%<extra></extra>\"))\n    fig.update_layout(template=temp, title=title,\n                      uniformtext_minsize=15, uniformtext_mode='hide')\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:00:12.921694Z","iopub.execute_input":"2022-04-18T05:00:12.922212Z","iopub.status.idle":"2022-04-18T05:00:12.937599Z","shell.execute_reply.started":"2022-04-18T05:00:12.922161Z","shell.execute_reply":"2022-04-18T05:00:12.936925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_fold = GroupKFold(n_splits=3)\nscaler = StandardScaler()\n\ntrain_seq_df=seq_df.merge(lbl, on='sequence')\ny=train_seq_df[['state']]\nX=train_seq_df.drop(['sequence','subject','state'], axis=1)\nX=pd.DataFrame(scaler.fit_transform(X))\nX_test=test_seq_df.drop(['sequence','subject'], axis=1)\nX_test=pd.DataFrame(scaler.transform(X_test))\n\ny_valid=[]\nglm_preds=[]\ntest_preds=[]\n\nfor fold, (train_index, val_index) in enumerate(k_fold.split(X, y, groups=train_seq_df.subject)):\n    \n    print(\"\\nFold {}\".format(fold+1))\n    \n    X_train, y_train = X.iloc[train_index,:], y.iloc[train_index,:].values\n    X_val, y_val = X.iloc[val_index,:], y.iloc[val_index,:].values\n    print(\"Train shape: {}, {}, Valid shape: {}, {}\".format(\n        X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n        \n    with tpu_strategy.scope():\n        \n        # Logistic Regression\n        mod = Sequential()\n        mod.add(Dense(1, activation='sigmoid', \n                      kernel_regularizer=regularizers.L2(1e-4), \n                      input_dim=X_train.shape[1]))\n\n        mod.compile(optimizer='adam', loss='binary_crossentropy', \n                    metrics=[metrics.AUC(name = 'auc')])\n\n        mod.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val), verbose=False)\n\n        glm_pred = mod.predict(X_val).squeeze()\n        auc_score=roc_auc_score(y_val, glm_pred)\n        print(\"Validation AUC = {:.4f}\".format(auc_score))\n\n        y_valid.append(y_val)\n        glm_preds.append(glm_pred)\n        test_preds.append(mod.predict(X_test).squeeze())\n    \n    del X_train, y_train, X_val, y_val\n    gc.collect()\n    \nplot_roc(y_val=y_valid, y_prob=glm_preds)    ","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T05:00:30.964827Z","iopub.execute_input":"2022-04-18T05:00:30.965114Z","iopub.status.idle":"2022-04-18T05:01:44.710725Z","shell.execute_reply.started":"2022-04-18T05:00:30.965083Z","shell.execute_reply":"2022-04-18T05:01:44.709818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Logistic Regression model has an Area Under the Curve (AUC) ranging from 0.78 to 0.8 in each fold.","metadata":{}},{"cell_type":"code","source":"sub_glm=sub.copy()\nsub_glm['state']=np.mean(test_preds, axis=0)\nsub_glm.to_csv(\"submission_glm.csv\", index=False)\nplot_predictions(df=sub_glm, title=\"Predicted Target Distribution,<br>Logistic Regression\")","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T05:01:52.222825Z","iopub.execute_input":"2022-04-18T05:01:52.223684Z","iopub.status.idle":"2022-04-18T05:01:52.305573Z","shell.execute_reply.started":"2022-04-18T05:01:52.22364Z","shell.execute_reply":"2022-04-18T05:01:52.304761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style=\"color:#46BAED\">Gradient Boosting</span></b>","metadata":{}},{"cell_type":"code","source":"train_seq_df=seq_df.merge(lbl, on='sequence')\ny=train_seq_df[['state']]\nX=train_seq_df.drop(['sequence','subject','state'], axis=1)\nX=pd.DataFrame(scaler.fit_transform(X))\nX_test=test_seq_df.drop(['sequence','subject'], axis=1)\nX_test=pd.DataFrame(scaler.transform(X_test))\n\ny_valid=[]\ngbm_probs=[]\ntest_preds=[]    \n                    \nfor fold, (train_index, val_index) in enumerate(k_fold.split(X, y, groups=train_seq_df.subject)):\n    \n    print(\"\\nFold {}\".format(fold+1))\n    \n    X_train, y_train = X.iloc[train_index,:], y.iloc[train_index,:].values\n    X_val, y_val = X.iloc[val_index,:], y.iloc[val_index,:].values\n    print(\"Train shape: {}, {}, Valid shape: {}, {}\".format(\n        X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n    \n    params = {'boosting_type': 'gbdt',\n              'n_estimators': 500,\n              'objective': 'binary',\n              'learning_rate': 0.1,\n              'colsample_bytree': 0.75,\n              'subsample': 0.75,\n              'metric': 'auc',\n              'random_state': 21}\n    \n    gbm = LGBMClassifier(**params).fit(X_train, y_train, \n                                       eval_set=[(X_train, y_train), (X_val, y_val)],\n                                       verbose=150,\n                                       eval_metric=['binary_logloss','auc'])\n    gbm_prob = gbm.predict_proba(X_val)[:,1]\n    y_valid.append(y_val)\n    gbm_probs.append(gbm_prob)\n    auc_score=roc_auc_score(y_val, gbm_prob)\n    print(\"Validation AUC = {:.4f}\".format(auc_score))\n    test_preds.append(gbm.predict_proba(X_test)[:,1])\n      \n    del X_train, y_train, X_val, y_val\n    gc.collect()  \n    \nplot_roc(y_val=y_valid, y_prob=gbm_probs)   ","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T05:02:00.216535Z","iopub.execute_input":"2022-04-18T05:02:00.21683Z","iopub.status.idle":"2022-04-18T05:02:18.397255Z","shell.execute_reply.started":"2022-04-18T05:02:00.216799Z","shell.execute_reply":"2022-04-18T05:02:18.396436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Gradient Boosting model improved the AUC to over 0.89 on the validation sets.","metadata":{}},{"cell_type":"code","source":"sub_gbm=sub.copy()\nsub_gbm['state']=np.mean(test_preds, axis=0)\nsub_gbm.to_csv(\"submission_gbm.csv\", index=False)\nplot_predictions(df=sub_gbm, title='Predicted Target Distribution<br>with Gradient Boosting')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T05:02:29.887349Z","iopub.execute_input":"2022-04-18T05:02:29.887657Z","iopub.status.idle":"2022-04-18T05:02:29.957281Z","shell.execute_reply.started":"2022-04-18T05:02:29.887628Z","shell.execute_reply":"2022-04-18T05:02:29.956554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style=\"color:#46BAED;margin:0;font-size:110%;text-align:left\">Bidirectional LSTM</span></b>\nThe last model I will test is a Bidirectional Long Short-Term Memory (LSTM) Network using the sensor information from all 60-second sequences. Bidirectional LSTMs are a type of Recurrent Neural Network that are trained on both directions of input sequences, from past to present (forward) and from present to past (backward). The LSTM model architecture is shown below.","metadata":{}},{"cell_type":"code","source":"y=np.array(lbl.state)\nX=train.drop(['sequence','subject','step','state'], axis=1)\nX=scaler.fit_transform(X)\nX=X.reshape(-1, 60, X.shape[1])\nX_test=test.drop(['sequence','subject','step'], axis=1)\nX_test=scaler.transform(X_test)\nX_test=X_test.reshape(-1, 60, X_test.shape[1])\ngroups=train.sequence.unique()\n\ny_valid=[]\nlstm_preds=[]\ntest_preds=[]    \n                    \nfor fold, (train_index, val_index) in enumerate(k_fold.split(X, y, groups=groups)):\n    \n    X_train, y_train = X[train_index], y[train_index]\n    X_val, y_val = X[val_index], y[val_index]\n    \n    with tpu_strategy.scope():\n        \n        x_input = Input(shape=(X.shape[-2:]))\n        x1 = Bidirectional(LSTM(units=512, return_sequences=True))(x_input)\n        \n        l1 = Bidirectional(LSTM(units=384, return_sequences=True))(x1)\n        l2 = Bidirectional(LSTM(units=384, return_sequences=True))(x_input)\n\n        c1 = Concatenate(axis=2)([l1,l2])\n\n        l3 = Bidirectional(LSTM(units=256, return_sequences=True))(c1)\n        l4 = Bidirectional(LSTM(units=256, return_sequences=True))(l2)\n        \n        c2 = Concatenate(axis=2)([l3,l4])\n        \n        l6 = GlobalMaxPooling1D()(c2)\n        l7 = Dense(units=128, activation='selu')(l6)\n        l8 = Dropout(0.05)(l7)\n        \n        output = Dense(1, activation='sigmoid')(l8)\n        \n        lr = ReduceLROnPlateau(monitor='val_auc', factor=0.4,  patience=3, verbose=True)\n        es = EarlyStopping(monitor='val_auc', mode='max', patience=5, \n                           restore_best_weights=True, verbose=True)\n        \n        model = Model(inputs=x_input, outputs=output, \n                      name='Bidirectional_LSTM')\n        \n        model.compile(optimizer='adam', \n                      loss='binary_crossentropy', \n                      metrics=[metrics.AUC(name = 'auc')])\n        \n        if fold==0:\n            model.summary() \n        \n        print(\"\\nFold {}\".format(fold+1))\n        print(\"Train shape: {}, {}, Valid shape: {}, {}\".format(\n            X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n        \n        model.fit(X_train, y_train,\n                  validation_data=(X_val, y_val), epochs=5, batch_size=128, \n                  callbacks=[es,lr], verbose=True)\n    \n        lstm_pred = model.predict(X_val).squeeze()\n        y_valid.append(y_val)\n        lstm_preds.append(lstm_pred)\n        test_preds.append(model.predict(X_test).squeeze())\n      \n    del X_train, y_train, X_val, y_val\n    gc.collect()\n\nplot_roc(y_val=y_valid, y_prob=lstm_preds) ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T05:02:47.586545Z","iopub.execute_input":"2022-04-18T05:02:47.586835Z","iopub.status.idle":"2022-04-18T05:08:46.713681Z","shell.execute_reply.started":"2022-04-18T05:02:47.586805Z","shell.execute_reply":"2022-04-18T05:08:46.712669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of the three models, the Bidirectional LSTM had the best performance with a cross-validation Area Under the Curve of over 0.96 in each fold.","metadata":{}},{"cell_type":"code","source":"sub_lstm=sub.copy()\nsub_lstm['state']=np.mean(test_preds, axis=0)\nsub_lstm.to_csv(\"submission_lstm.csv\", index=False)\nplot_predictions(df=sub_lstm, title='LSTM Predicted Target Distribution')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T05:09:25.763659Z","iopub.execute_input":"2022-04-18T05:09:25.763976Z","iopub.status.idle":"2022-04-18T05:09:25.8316Z","shell.execute_reply.started":"2022-04-18T05:09:25.763942Z","shell.execute_reply":"2022-04-18T05:09:25.83071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <div style=\"color:#46BAED;margin:0;font-size:100%;text-align:center\">Thank you for reading!</div>","metadata":{}}]}